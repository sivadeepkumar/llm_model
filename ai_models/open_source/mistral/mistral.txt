GPT4All Mistral Model - API Setup
This repository contains the necessary files and instructions to run the Mistral model using the GPT4All API.

Setup
Download the Mistral model from GPT4All(Using --> https://gpt4all.io/index.html). The model file you need is mistral-7b-instruct-v0.1.Q4_0.gguf.

Place the downloaded model file (mistral-7b-instruct-v0.1.Q4_0.gguf) in the following path within your project repository: gpt4all-api/gpt4all_api/models.

Update the .env file in your project repository and add the following line:
MODEL_BIN = "mistral-7b-instruct-v0.1.Q4_0"

Next step:
Navigate to the settings file (gpt4all-api/gpt4all_api/app/api_v1/settings.py) and ensure the following settings are added or modified:

class Settings(BaseSettings):
    app_environment = 'dev'
    model: str = 'mistral-7b-instruct-v0.1.Q4_0'
    gpt4all_path: str = '/models'
    inference_mode: str = "cpu"
    hf_inference_server_host: str = "http://gpt4all_gpu:80/generate"  # Change if using GPU
    sentry_dns: str = None
    temp: float = 0.18
    top_p: float = 1.0
    top_k: int = 50
    repeat_penalty: float = 1.18

To Running the Project we need Docker Setup

Ensure Docker is installed on your machine.

Change your working directory to gpt4all/gpt4all-api.

Build the FastAPI docker image (only required on initial build or when dependencies change):


DOCKER_BUILDKIT=1 docker build -t gpt4all_api --progress plain -f gpt4all_api/Dockerfile.buildkit .

Starting the Backend:-
To start the backend (API and locally hosted GPU inference server), run:
docker compose up --build


If you want to run the API without the GPU inference server, use:
docker compose up --build gpt4all_api


For running the API with the GPU inference server, edit the .env file with necessary environment variables and run:
docker compose --env-file .env up --build


Development and Testing
To run the API in development mode with hot-reloading:
docker compose up --build

Edit files in the app directory as needed. The API will hot-reload on changes.
Run unit tests with:


Accessing API Documentation
Once the FastAPI app is running, access its documentation and test the endpoints by visiting:
localhost:80/docs


"""
Send a completion request to a GPT4All API endpoint.

Parameters:
- model_name (str): The name of the GPT4All model to use for completion.
- prompt_text (str): The input prompt text for the model.
- max_tokens (int): The maximum number of tokens for completion (default is 50).
- temperature (float): The temperature parameter for controlling randomness in generation (default is 0.7).
- **kwargs: Additional keyword arguments for the request payload.

Returns:
- dict: The JSON response containing the model's completion.
"""